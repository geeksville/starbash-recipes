
[repo]
kind = "recipe"

[recipe]
author.name = "FIXMEsirilsomeone"
author.email = "FIXMEsirilsomeone"

# Lower priority recipes are tried to match first
# - we want this to match after the osc_dual_duo has a chance
priority = 10

# all sb.toml files can optionally contain a version section.  if version of the running starbash app is out of bounds a warning message will be printed
# to the user and the file will be ignored for future processing.
#[recipe.require.version]
#min = "0.1.0"
#max = "4.5.8"

[[stages2]] # FIXME, until I'm ready to remove processing_classic, I'm temporarily calling the 'new style' stages stages2

name = "light"
description = "Extract OSC HaOiii channels"
# disabled = true # can be used to prevent considering this stage (for debugging or whatever)

# We should normally be inheriting this from a common base stage definition
tool.name = "siril"

# FIXME allow some sort of inheritence for these stage definitions.  because for most 'session' stages the only difference will be the script
# possibly a general 'repo' level feature of:
# inherit... does a get() then a deep copy of the named node?
# this would allow most stage definitions to just be an inhert + a short script string
#
#[stages.inherit]
# if repo is not specified the current repo is assumed?
#repo = "http://github.com/geeksville/starbash-recipes/osc_single_duo"
#node = "stage_def.foo"

# Notice: the following script should be shared between dual duo **and** single duo workflows.  We shouldn't need to delcare it twice.
script = '''
    # Create a sequence from the raw light frames, seq file goes to process_dir
    link {light_base} -out={process_dir}
    cd {process_dir}

    # Calibrate the light frames using master bias and flat
    calibrate {light_base} -bias={input["bias"].full} -flat={input["flat"].full} -cfa -equalize_cfa

    # Remove background gradient on a per-frame basis (generates bkg_pp_light.seq)
    seqsubsky pp_{light_base} 1

    # we only do this step for duo filters
    seqextract_HaOIII bkg_pp_{light_base} -resample=ha
    # Note: the sequextract rule generates Ha_bkg_{light_base}_.seq and OIII_bkg_pp_{light_base}_.seq
    '''

# recipes/stages can explicitly add new entries to the context dict.  This is useful for making short
# readable keys that can be used in scripts.  For example here we are defining 'light_base' to be the base name
# of the input light frames for this session.
[stages2.context]
light_base = """{input["light"].base}"""
# Note: process_dir is provided automatically by the stage runner tasks

# temporaries = ["pp_{light_base}"]

# inputs.name could eventually be optionally be specified.  But currently they are alway auto generated.
# in this case the 'name' will be "light" and therefore context["input"]["light"] will contain the information
# about this input.
# In the case of session based inputs the name will will be coded based on a combination of the session ID and the type.
# context["input"]["light"].base = "light_s{session["id"]}" # e.g. light_s23 for session ID 23
[[stages2.inputs]]
# Auto find input light frames for the current session
# Look for files in input repos, finding them by using the "relative" tag they contain
kind = "session" # Formerly called "repo"
type = "light"   # we are only applicable for sessions of type light frame

# 'requires' expressions say required metadata for this input.  All requires blocks must be satisfied for this input to be considered.
# Note: kind is designed so that eventually we could programmtically add new types of require blocks - even from recipes?
# Note: currently 'requires' is only allowed for session inputs, but we could generalize it in the future if needed.
[[stages2.inputs.requires]]
kind = "metadata"
name = "filter"
value = [
    "HaOiii",
    "SiiOiii",
] # if a list is given it means OR.  If you want to do and use multiple requires blocks

[[stages2.inputs.requires]]
kind = "min_count"
value = 2          # siril needs at least 2 frames to stack, raise an exception if not enough frames found

[[stages2.inputs.requires]]
kind = "camera"
value = "color" # only color cameras supported for osc dual duo

[[stages2.inputs]]
kind = "master" # Formerly called "repo"
type = "bias"   # we are only applicable for sessions of type light frame
# default input name will be context["input"]["master_bias"]

[[stages2.inputs]]
kind = "master" # Formerly called "repo"
type = "flat"   # we are only applicable for sessions of type light frame
# default input name will be context["input"]["master_flat"]

[[stages2.outputs]]

# kind="job" means this file will be generated in the shared job directory used for all tasks.  Normally this
# directory is keyed based on a target name (we might be processing multiple sessions on that one target)
kind = "job"

# this name is combined with the job directory to construct a full absolute path for the output file
# note - this name might include multiplex based keys like _s23.  It is also allowed to use variable names from the context.
#
# TBD: is it possible to allow wildcards in this string?  so that doit 'clean' operations would automatically know all
# of the output files (including the subframes).
#
# Note: this example generates two output files - one for Ha and one for OIII.  But normally name can be either a string or a list of
# strings.
name = ["Ha_bkg_pp_{light_base}_.seq", "OIII_bkg_pp_{light_base}_.seq"]

[[stages2]]

# this stage is considered if the previous stage in this same array
# was run.  It must be run inside the same tempdir (so that files from previous stage are available)

name = "stack"
description = "Stack OSC dual duo (HaOiii + SiiOiii) filter data: with separate Ha, Oiii and Sii channels"

tool.name = "python"

# this script requires dual filters
auto.require.filter = ["SiiOiii", "HaOiii"]

# if not specified starbash.py used
# script-file = "script.py"

# or inline python code can be provided here.  In this case I'm using some python
# code I'm temporarily sharing from the main project...
script = '''
    from starbash.recipes import osc
    osc.logger = logger
    osc.context = context
    osc.osc_process(has_ha_oiii=True, has_sii_oiii=True)
    '''

[stages2.context]

# where to put final results
# make a common base stage importable def for "final_stage"?
results_dir = """{context["output"]["processed"].base}"""

[[stages2.inputs]]
kind = "job"
name = "ha"

[[stages2.inputs.requires]]
kind = "metadata"
name = "filter"
value = ["HaOiii"]

# FIXME: "Ha_bkg_pp_*_.seq" can't work - because 'light_base' is session specific, we really instead want to do a glob_match of some sort
# but globs() can't be evaluated until after we've run at least the preceeding tasks (so they can make the files).
#
# Insted we **guarantee** that the "job" input will be auto populated with the expected output files for all the sessions that
# were multiplexed at the previous stage
# name = ["Ha_bkg_pp_{multiplexed}_.seq", "OIII_bkg_pp_{multiplexed}_.seq"]

# Notice how we have TWO inputs, each of kind job but with a DIFFERENT require filter block.  This will auto provision our context
# with two input dictionaries.  One named "sii" and the other named "ha".  The names picked for "job" kind can be anything you want -
# they are only used to name the input dicts in context["input"].

# the metadata filtering is performed on the INPUT files for the previous stage - because those are the only ones that exist at the
# point we are deciding which inputs to provide here.  If the metadata matches for a session/task/multiplex for the prior task, all of
# that task's outputs will be grouped into the corresponding named job.

[[stages2.inputs]]
kind = "job"
name ="sii"

[[stages2.inputs.requires]]
kind = "metadata"
name = "filter"
value = ["SiiOiii"]

# Based on the following definitions in the stage toml file...
# Note: outputs will be prefilled in the context so that the script can use them
[[stages2.outputs]]
# the 'processed' output points to a directory in the 'processed' repo but named based on the target we are trying to build for
# see above for a rule that automatically populates the results_dir shorthand
kind = "processed"
name = [
    "starbash.toml",     # FIXME: automatically add starbash.toml as an output for any 'processed' outputs?
    "stacked_Ha.fits",
    "stacked_Oiii.fits",
    "stacked_Sii.fits",
]
