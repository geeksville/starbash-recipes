
[repo]
kind = "recipe"

[recipe]
# This recipe is heavily based OSC_Extract_HaOIII.ssf in Siril!
author.name = "Siril Developers"

[[stages]]

# this stage is considered if the previous stage in this same array
# was run.  It must be run inside the same tempdir (so that files from previous stage are available)

name = "stack_dual_duo"
description = "Stack OSC dual duo (HaOiii + SiiOiii) filter data: with separate Ha, Oiii and Sii channels"
tool.name = "python"
priority = 330

# if not specified starbash.py used
# script-file = "script.py"

# or inline python code can be provided here.  In this case I'm using some python
# code I'm temporarily sharing from the main project...
script = '''
    from starbash.recipes import osc
    osc.logger = logger
    osc.context = context
    osc.osc_process(has_ha_oiii=True, has_sii_oiii=True)
    '''

[[stages.inputs]]
kind = "job"
name = "ha" # Group all inputs of this kind under the name "ha"
after = "seqextract_haoiii"   # the name of the step we want to follow

[[stages.inputs.requires]]
kind = "metadata"
name = "filter"
value = ["HaOiii"]

# Insted we **guarantee** that the "job" input will be auto populated with the expected output files for all the sessions that
# were multiplexed at the previous stage
#
# Notice how we have TWO inputs, each of kind job but with a DIFFERENT require filter block.  This will auto provision our context
# with two input dictionaries.  One named "sii" and the other named "ha".  The names picked for "job" kind can be anything you want -
# they are only used to name the input dicts in context["input"].
#
# the metadata filtering is performed on the INPUT files for the previous stage - because those are the only ones that exist at the
# point we are deciding which inputs to provide here.  If the metadata matches for a session/task/multiplex for the prior task, all of
# that task's outputs will be grouped into the corresponding named job.

[[stages.inputs]]
kind = "job"
name ="sii"
after = "seqextract_haoiii"   # the name of the step we want to follow

[[stages.inputs.requires]]
kind = "metadata"
name = "filter"
value = ["SiiOiii"]

[[stages.inputs.requires]]
kind = "min_count"
value = 1          # At this point if we don't have any valid inputs, reject this stage from consideration

# Based on the following definitions in the stage toml file...
# Note: outputs will be prefilled in the context so that the script can use them
[[stages.outputs]]
# the 'processed' output points to a directory in the 'processed' repo but named based on the target we are trying to build for
# see above for a rule that automatically populates the results_dir shorthand
kind = "processed"
name = [
    "stacked_Ha.fits",
    "stacked_OIII.fits",
    "stacked_Sii.fits",
]
