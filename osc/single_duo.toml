
[repo]
kind = "recipe"

[recipe]
# This recipe is heavily based OSC_Extract_HaOIII.ssf in Siril!
author.name = "Siril Developers"

[[stages]]

# this stage is considered if the previous stage in this same array
# was run.  It must be run inside the same tempdir (so that files from previous stage are available)

name = "stack_single_duo"
description = "Stack OSC single duo (HaOiii) filter data: with separate Ha and Oiii channels"
tool.name = "python"
priority = 320

# if not specified starbash.py used
# script-file = "script.py"

# or inline python code can be provided here.  In this case I'm using some python
# code I'm temporarily sharing from the main project...
script = '''
    from starbash.recipes import osc
    osc.logger = logger
    osc.context = context
    osc.osc_process(has_ha_oiii=True, has_sii_oiii=False)
    '''

[[stages.inputs]]
kind = "job"
name = "ha" # Group all inputs of this kind under the name "ha"
after = "seqextract_haoiii"   # the name of the step we want to follow
merge_to = "in"

[[stages.inputs.requires]]
kind = "metadata"
name = "filter"
value = ["HaOiii"]

[[stages.inputs.requires]]
kind = "min_count"
value = 1          # At this point if we don't have any valid inputs, reject this stage from consideration

# Based on the following definitions in the stage toml file...
# Note: outputs will be prefilled in the context so that the script can use them
[[stages.outputs]]
# the 'processed' output points to a directory in the 'processed' repo but named based on the target we are trying to build for
# see above for a rule that automatically populates the results_dir shorthand
kind = "processed"
name = [
    "stacked_Ha.fits",
    "stacked_OIII.fits"
]
